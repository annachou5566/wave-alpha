import json
import os
import time
import random
from datetime import datetime
from dotenv import load_dotenv
import requests # D√πng requests v√¨ ƒë√£ c√≥ Proxy lo ph·∫ßn IP
import cloudscraper # D·ª± ph√≤ng n·∫øu c·∫ßn direct

# --- 1. C·∫§U H√åNH & K·∫æT N·ªêI ---
load_dotenv()


PROXY_WORKER_URL = os.getenv("PROXY_WORKER_URL")

API_AGG_TICKER = os.getenv("BINANCE_INTERNAL_AGG_API")
API_AGG_KLINES = os.getenv("BINANCE_INTERNAL_KLINES_API")
API_PUBLIC_SPOT = "https://api.binance.com/api/v3/exchangeInfo"

# Ch·ªâ qu√©t chi ti·∫øt Top N token volume to nh·∫•t
TOP_TOKEN_LIMIT = 60 

# Scraper d·ª± ph√≤ng (d√πng khi kh√¥ng c√≥ Proxy)
scraper = cloudscraper.create_scraper(
    browser={'browser': 'chrome', 'platform': 'windows', 'desktop': True}
)

# Headers d·ª± ph√≤ng
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
    "Referer": "https://www.binance.com/en/alpha",
    "Origin": "https://www.binance.com",
    "Accept": "application/json"
}

OUTPUT_FILE = "public/data/market-data.json"
os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)

ACTIVE_SPOT_SYMBOLS = set()
OLD_DATA_MAP = {}

# --- 2. C√ÅC H√ÄM G·ªåI API (CORE) ---

def fetch_via_proxy_or_direct(target_url, retries=3):
    """
    H√†m th√¥ng minh: T·ª± ƒë·ªông ch·ªçn ƒëi qua Proxy (n·∫øu c√≥) ho·∫∑c ƒëi th·∫≥ng.
    """
    use_proxy = True if PROXY_WORKER_URL and "workers.dev" in PROXY_WORKER_URL else False
    
    for i in range(retries):
        try:
            if use_proxy:
                # G·ªçi qua Cloudflare Worker
                proxy_endpoint = f"{PROXY_WORKER_URL}?url={target_url}"
                # Timeout d√†i (30s) ƒë·ªÉ ch·ªù Worker x·ª≠ l√Ω delay
                res = requests.get(proxy_endpoint, timeout=30)
            else:
                # G·ªçi tr·ª±c ti·∫øp (D·ªÖ b·ªã 403)
                res = scraper.get(target_url, headers=HEADERS, timeout=15)

            # X·ª≠ l√Ω k·∫øt qu·∫£
            if res.status_code == 200:
                return res.json()
            elif res.status_code == 403:
                print(f"‚õî B·ªã ch·∫∑n (403). ƒê·ª£i 5s...")
                time.sleep(5)
            elif res.status_code == 429:
                print(f"‚ö†Ô∏è Rate Limit (429). ƒê·ª£i 10s...")
                time.sleep(10)
            else:
                time.sleep(1)
        except Exception as e:
            print(f"‚ö†Ô∏è L·ªói k·∫øt n·ªëi ({'Proxy' if use_proxy else 'Direct'}): {e}")
            time.sleep(1)
            
    return None

def safe_float(v):
    try: return float(v) if v else 0.0
    except: return 0.0

def load_old_data():
    if os.path.exists(OUTPUT_FILE):
        try:
            with open(OUTPUT_FILE, 'r', encoding='utf-8') as f:
                data = json.load(f)
                return {t['id']: t for t in data.get('tokens', [])}
        except: pass
    return {}

def get_active_spot_symbols():
    # Spot API public, g·ªçi tr·ª±c ti·∫øp cho nhanh, √≠t khi ch·∫∑n
    try:
        res = scraper.get(API_PUBLIC_SPOT, timeout=10)
        if res.status_code == 200:
            data = res.json()
            symbols = {s["baseAsset"] for s in data.get("symbols", []) if s["status"] == "TRADING"}
            return symbols
    except: pass
    return set()

# --- 3. LOGIC L·∫§Y D·ªÆ LI·ªÜU CHI TI·∫æT ---

def fetch_daily_utc_stats(chain_id, contract_addr):
    d_total = 0.0
    d_limit = 0.0
    d_market = 0.0 
    
    # URL G·ªëc
    base_binance_url = f"{API_AGG_KLINES}?chainId={chain_id}&interval=1d&limit=5&tokenAddress={contract_addr}"
    
    # N·∫øu kh√¥ng d√πng Proxy, c·∫ßn delay ƒë·ªÉ tr√°nh spam
    if not PROXY_WORKER_URL:
        time.sleep(random.uniform(2, 4)) 

    # 1. LIMIT
    res_limit = fetch_via_proxy_or_direct(f"{base_binance_url}&dataType=limit")
    if res_limit and res_limit.get("data"):
        k = res_limit["data"]["klineInfos"]
        if k: d_limit = safe_float(k[-1][5])

    # 2. MARKET (ON-CHAIN)
    res_market = fetch_via_proxy_or_direct(f"{base_binance_url}&dataType=market")
    if res_market and res_market.get("data"):
        k = res_market["data"]["klineInfos"]
        if k: d_market = safe_float(k[-1][5])

    # 3. AGGREGATE (TOTAL)
    res_total = fetch_via_proxy_or_direct(f"{base_binance_url}&dataType=aggregate")
    if res_total and res_total.get("data"):
        k = res_total["data"]["klineInfos"]
        if k: d_total = safe_float(k[-1][5])

    # Fallback Logic
    if d_total < (d_limit + d_market):
        d_total = d_limit + d_market
    
    return d_total, d_limit, d_market

def get_sparkline_data(chain_id, contract_addr):
    target_url = f"{API_AGG_KLINES}?chainId={chain_id}&interval=1d&limit=20&tokenAddress={contract_addr}&dataType=aggregate"
    res = fetch_via_proxy_or_direct(target_url)
    if res and res.get("data") and res["data"].get("klineInfos"):
        return [{"p": safe_float(k[4]), "v": safe_float(k[5])} for k in res["data"]["klineInfos"]]
    return []

# --- 4. X·ª¨ L√ù T·ª™NG TOKEN (FULL FIELDS) ---

def process_token_smart(item, is_vip=False):
    # Logic: Ch·ªâ g·ªçi API chi ti·∫øt n·∫øu l√† VIP
    should_fetch_details = is_vip 
    
    aid = item.get("alphaId")
    vol_24h = safe_float(item.get("volume24h"))
    symbol = item.get("symbol")
    contract = item.get("contractAddress")
    chain_id = item.get("chainId")

    # --- STATUS LOGIC (GI·ªÆ NGUY√äN) ---
    is_offline = item.get("offline", False)
    is_listing_cex = item.get("listingCex", False)
    status = "ALPHA"
    if is_offline:
        if is_listing_cex is True or symbol in ACTIVE_SPOT_SYMBOLS:
            status = "SPOT"
            is_listing_cex = True
        else:
            status = "DELISTED"
    else:
        status = "ALPHA"

    # --- INIT VARS ---
    daily_total = 0.0
    daily_limit = 0.0
    daily_onchain = 0.0
    chart_data = []

    # --- CACHE LOGIC ---
    old_entry = OLD_DATA_MAP.get(aid)
    
    # N·∫øu kh√¥ng ph·∫£i VIP, ∆∞u ti√™n l·∫•y d·ªØ li·ªáu c≈© ƒë·ªÉ hi·ªÉn th·ªã cho ƒë·∫πp
    if old_entry and not should_fetch_details:
        if old_entry.get("volume"):
             daily_limit = safe_float(old_entry["volume"].get("daily_limit"))
             daily_onchain = safe_float(old_entry["volume"].get("daily_onchain"))
             # L·∫•y lu√¥n total c≈© n·∫øu n√≥ h·ª£p l√Ω
             if safe_float(old_entry["volume"].get("daily_total")) > 0:
                 daily_total = safe_float(old_entry["volume"].get("daily_total"))
             chart_data = old_entry.get("chart", [])

    # --- FETCHING (CH·ªà VIP M·ªöI G·ªåI M·ªöI) ---
    if should_fetch_details and vol_24h > 0 and contract and chain_id:
        print(f"üì° Fetching: {symbol}...")
        d_total, d_limit, d_market = fetch_daily_utc_stats(chain_id, contract)
        
        daily_limit = d_limit
        daily_onchain = d_market
        daily_total = d_total if d_total >= (d_limit + d_market) else (d_limit + d_market)
        chart_data = get_sparkline_data(chain_id, contract)
    
    # Fallback cho daily_total n·∫øu ch∆∞a c√≥
    if daily_total == 0:
        daily_total = vol_24h

    # --- RETURN FULL DATA (KH√îNG B·ªé S√ìT FIELD N√ÄO) ---
    return {
        "id": aid,
        "symbol": symbol,
        "name": item.get("name"),
        "icon": item.get("iconUrl"),
        "chain": item.get("chainName", ""),
        "chain_icon": item.get("chainIconUrl"),
        "contract": contract,
        
        # C√°c tr∆∞·ªùng quan tr·ªçng gi·ªØ nguy√™n
        "offline": is_offline,
        "listingCex": is_listing_cex,
        "status": status,
        "onlineTge": item.get("onlineTge", False),
        "onlineAirdrop": item.get("onlineAirdrop", False),
        "mul_point": safe_float(item.get("mulPoint")),
        "listing_time": item.get("listingTime", 0),
        "tx_count": safe_float(item.get("count24h")),
        
        "price": safe_float(item.get("price")),
        "change_24h": safe_float(item.get("percentChange24h")),
        "liquidity": safe_float(item.get("liquidity")),
        "market_cap": safe_float(item.get("marketCap")),
        
        "volume": {
            "rolling_24h": vol_24h,
            "daily_total": daily_total,
            "daily_limit": daily_limit,
            "daily_onchain": daily_onchain
        },
        "chart": chart_data
    }

# --- 5. MAIN ---
def fetch_data():
    global ACTIVE_SPOT_SYMBOLS, OLD_DATA_MAP
    start_time = time.time()
    
    mode_str = "PROXY" if PROXY_WORKER_URL else "DIRECT (UNSAFE)"
    print(f"üõ°Ô∏è [MODE: {mode_str}] B·∫Øt ƒë·∫ßu qu√©t d·ªØ li·ªáu Alpha...")
    
    if not API_AGG_TICKER:
        print("‚ùå L·ªói: Ch∆∞a c·∫•u h√¨nh bi·∫øn m√¥i tr∆∞·ªùng API_AGG_TICKER")
        return

    OLD_DATA_MAP = load_old_data()
    ACTIVE_SPOT_SYMBOLS = get_active_spot_symbols()
    
    # L·∫•y danh s√°ch t·ªïng (G·ªçi qua Proxy/Direct)
    raw_res = fetch_via_proxy_or_direct(API_AGG_TICKER)
    if not raw_res:
        print("‚ùå Kh√¥ng l·∫•y ƒë∆∞·ª£c danh s√°ch Token g·ªëc!")
        return
    raw_data = raw_res.get("data", [])

    # L·ªçc & S·∫Øp x·∫øp
    all_tokens = [t for t in raw_data if safe_float(t.get("volume24h")) > 0]
    all_tokens.sort(key=lambda x: safe_float(x.get("volume24h")), reverse=True)
    
    # Chia nh√≥m VIP v√† Th∆∞·ªùng
    vip_tokens = all_tokens[:TOP_TOKEN_LIMIT]
    normal_tokens = all_tokens[TOP_TOKEN_LIMIT:]

    results = []

    # 1. VIP (G·ªçi API chi ti·∫øt)
    print(f"üíé X·ª≠ l√Ω {len(vip_tokens)} Token VIP...")
    for t in vip_tokens:
        res = process_token_smart(t, is_vip=True)
        if res: results.append(res)
    
    # 2. Normal (D√πng data c∆° b·∫£n + Cache c≈©)
    print(f"‚ö° X·ª≠ l√Ω nhanh {len(normal_tokens)} Token th∆∞·ªùng...")
    for t in normal_tokens:
        res = process_token_smart(t, is_vip=False)
        if res: results.append(res)

    # 3. R√°c (Volume 0)
    zero_tokens = [t for t in raw_data if safe_float(t.get("volume24h")) == 0]
    for t in zero_tokens:
        res = process_token_smart(t, is_vip=False)
        if res: results.append(res)

    # S·∫Øp x·∫øp output cu·ªëi c√πng
    results.sort(key=lambda x: x["volume"]["daily_total"], reverse=True)

    data = {
        "last_updated": datetime.now().strftime("%H:%M %d/%m"),
        "total_tokens": len(results),
        "tokens": results
    }
    
    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
        
    print(f"‚úÖ HO√ÄN T·∫§T! Th·ªùi gian: {time.time() - start_time:.2f}s | Token: {len(results)}")

if __name__ == "__main__":
    fetch_data()