import requests
import json
import os
import time
from datetime import datetime
import concurrent.futures
from dotenv import load_dotenv

# --- 1. C·∫§U H√åNH & B·∫¢O M·∫¨T ---
load_dotenv()

# L·∫•y API n·ªôi b·ªô t·ª´ bi·∫øn m√¥i tr∆∞·ªùng (Tuy·ªát ƒë·ªëi kh√¥ng hardcode)
API_AGG_TICKER = os.getenv("BINANCE_INTERNAL_AGG_API")
API_AGG_KLINES = os.getenv("BINANCE_INTERNAL_KLINES_API")

# API C√¥ng khai (Public) ƒë·ªÉ ki·ªÉm tra ch√©o - An to√†n, kh√¥ng c·∫ßn key
API_PUBLIC_SPOT = "https://api.binance.com/api/v3/exchangeInfo"

if not API_AGG_TICKER or not API_AGG_KLINES:
    print("‚ùå L·ªñI: Thi·∫øu API Binance trong file .env")
    exit()

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Referer": "https://www.binance.com/en/alpha",
    "Origin": "https://www.binance.com",
    "client-type": "web"
}

OUTPUT_FILE = "public/data/market-data.json"
os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)

MAX_WORKERS = 5 # Gi·ªØ 5 lu·ªìng ƒë·ªÉ ·ªïn ƒë·ªãnh, kh√¥ng b·ªã rate limit

# Bi·∫øn to√†n c·ª•c ƒë·ªÉ l∆∞u cache v√† danh s√°ch Spot th·ª±c t·∫ø
ACTIVE_SPOT_SYMBOLS = set()
OLD_DATA_MAP = {} # L∆∞u d·ªØ li·ªáu c≈© ƒë·ªÉ t√°i s·ª≠ d·ª•ng

# --- 2. C√ÅC H√ÄM B·ªî TR·ª¢ (UTILITIES) ---

def safe_float(v):
    try: return float(v) if v else 0.0
    except: return 0.0

def load_old_data():
    """
    ƒê·ªçc d·ªØ li·ªáu t·ª´ file JSON c≈© ƒë·ªÉ l√†m Cache.
    Gi√∫p tr√°nh g·ªçi l·∫°i API cho c√°c token ƒë√£ 'ch·ªët s·ªï' (Spot/Delisted).
    """
    if os.path.exists(OUTPUT_FILE):
        try:
            with open(OUTPUT_FILE, 'r', encoding='utf-8') as f:
                data = json.load(f)
                # T·∫°o Map: { "ALPHA_123": { ...token_data... } }
                return {t['id']: t for t in data.get('tokens', [])}
        except: pass
    return {}

def get_active_spot_symbols():
    """
    G·ªçi API Public Spot ƒë·ªÉ l·∫•y danh s√°ch 'S·ª± Th·∫≠t'.
    D√πng ƒë·ªÉ s·ª≠a l·ªói khi API Alpha b√°o sai tr·∫°ng th√°i (VD: FOGO).
    """
    print("üåç ƒêang l·∫•y danh s√°ch Spot Trading th·ª±c t·∫ø (Public API)...")
    try:
        res = requests.get(API_PUBLIC_SPOT, timeout=10)
        if res.status_code == 200:
            data = res.json()
            symbols = set()
            for s in data.get("symbols", []):
                if s.get("status") == "TRADING":
                    symbols.add(s.get("baseAsset")) 
            print(f"‚úÖ ƒê√£ t√¨m th·∫•y {len(symbols)} token ƒëang giao d·ªãch Spot.")
            return symbols
    except Exception as e:
        print(f"‚ö†Ô∏è Kh√¥ng g·ªçi ƒë∆∞·ª£c API Spot (S·∫Ω d√πng d·ªØ li·ªáu g·ªëc): {e}")
    return set()

def get_usd_from_kline(kline_array):
    if not kline_array or not isinstance(kline_array, list): return 0.0
    length = len(kline_array)
    try:
        # Index 5 l√† Volume USD (cho c·∫£ Limit v√† Aggregate hi·ªán t·∫°i)
        if length >= 6: return safe_float(kline_array[5])
        elif length >= 8: return safe_float(kline_array[7])
    except: pass
    return 0.0

def fetch_with_retry(url, retries=3):
    for i in range(retries):
        try:
            res = requests.get(url, headers=HEADERS, timeout=5)
            if res.status_code == 200:
                return res.json()
            elif res.status_code == 429:
                time.sleep(2) # B·ªã ch·∫∑n nh·∫π th√¨ ngh·ªâ ch√∫t
        except:
            time.sleep(0.5)
    return None

def fetch_daily_utc_stats(chain_id, contract_addr):
    d_total = 0.0
    d_limit = 0.0
    
    # DataType = aggregate
    url_total = f"{API_AGG_KLINES}?chainId={chain_id}&interval=1d&limit=1&tokenAddress={contract_addr}&dataType=aggregate"
    res_total = fetch_with_retry(url_total)
    if res_total and res_total.get("data") and res_total["data"].get("klineInfos"):
        d_total = get_usd_from_kline(res_total["data"]["klineInfos"][0])

    # DataType = limit
    url_limit = f"{API_AGG_KLINES}?chainId={chain_id}&interval=1d&limit=1&tokenAddress={contract_addr}&dataType=limit"
    res_limit = fetch_with_retry(url_limit)
    if res_limit and res_limit.get("data") and res_limit["data"].get("klineInfos"):
        d_limit = get_usd_from_kline(res_limit["data"]["klineInfos"][0])
    
    return d_total, d_limit

def get_sparkline_data(chain_id, contract_addr):
    chart = []
    url = f"{API_AGG_KLINES}?chainId={chain_id}&interval=1d&limit=7&tokenAddress={contract_addr}&dataType=aggregate"
    res = fetch_with_retry(url, retries=2)
    if res and res.get("data") and res["data"].get("klineInfos"):
        chart = [safe_float(k[4]) for k in res["data"]["klineInfos"]]
    return chart

# --- 3. WORKER (L√ïI X·ª¨ L√ù) ---
def process_token_smart(item):
    """
    H√†m x·ª≠ l√Ω th√¥ng minh:
    1. Check Cache c≈© -> N·∫øu ƒë√£ Spot/Delisted th√¨ b·ªè qua g·ªçi API n·∫∑ng.
    2. Check Cross-Check -> S·ª≠a l·ªói tr·∫°ng th√°i.
    3. Ch·ªâ g·ªçi API Klines cho token c·∫ßn thi·∫øt.
    """
    aid = item.get("alphaId")
    if not aid: return None

    # L·∫•y th√¥ng tin c∆° b·∫£n t·ª´ API T·ªïng (API Nh·∫π)
    vol_24h = safe_float(item.get("volume24h"))
    price = safe_float(item.get("price"))
    change_24h = safe_float(item.get("percentChange24h"))
    tx_count = safe_float(item.get("count24h"))
    liquidity = safe_float(item.get("liquidity"))
    market_cap = safe_float(item.get("marketCap"))
    
    contract = item.get("contractAddress")
    chain_id = item.get("chainId") 
    chain_name = item.get("chainName", "")
    symbol = item.get("symbol")

    # --- LOGIC X√ÅC ƒê·ªäNH TR·∫†NG TH√ÅI (STATUS) ---
    is_offline = item.get("offline", False)
    is_listing_cex = item.get("listingCex", False)
    
    status = "ALPHA"
    if is_offline is True:
        if is_listing_cex is True:
            status = "SPOT"
        else:
            # CROSS-CHECK: N·∫øu Alpha b·∫£o Delisted nh∆∞ng Spot c√≥ -> S·ª≠a th√†nh SPOT
            if symbol in ACTIVE_SPOT_SYMBOLS:
                status = "SPOT"
                is_listing_cex = True 
            else:
                status = "DELISTED"

    # --- LOGIC CACHING (QUAN TR·ªåNG) ---
    # Ki·ªÉm tra xem token n√†y ƒë√£ c√≥ trong file c≈© ch∆∞a
    old_data = OLD_DATA_MAP.get(aid)
    
    daily_total = 0.0
    daily_limit = 0.0
    daily_onchain = 0.0
    chart_data = []
    
    # QUY·∫æT ƒê·ªäNH: C√ì G·ªåI API CHI TI·∫æT HAY KH√îNG?
    should_fetch_details = False
    
    if status == "ALPHA":
        # N·∫øu ƒëang ch·∫°y gi·∫£i -> Lu√¥n ph·∫£i c·∫≠p nh·∫≠t m·ªõi
        should_fetch_details = True
    else:
        # N·∫øu ƒë√£ SPOT ho·∫∑c DELISTED
        if old_data and old_data.get("status") == status:
            # Tr·∫°ng th√°i kh√¥ng ƒë·ªïi -> D√πng l·∫°i d·ªØ li·ªáu c≈© (Chart, Volume gi·∫£i)
            # Ch·ªâ c·∫≠p nh·∫≠t Gi√° & Vol 24h t·ª´ API Nh·∫π
            daily_total = safe_float(old_data["volume"].get("daily_total"))
            daily_limit = safe_float(old_data["volume"].get("daily_limit"))
            daily_onchain = safe_float(old_data["volume"].get("daily_onchain"))
            chart_data = old_data.get("chart", [])
            should_fetch_details = False # Ti·∫øt ki·ªám API
        else:
            # Tr·∫°ng th√°i m·ªõi ƒë·ªïi (VD: M·ªõi chuy·ªÉn t·ª´ Alpha sang Spot) -> G·ªçi 1 l·∫ßn ƒë·ªÉ ch·ªët
            should_fetch_details = True

    # TH·ª∞C THI G·ªåI API (N·∫æU C·∫¶N)
    if should_fetch_details and vol_24h > 0 and contract and chain_id:
        daily_total, daily_limit = fetch_daily_utc_stats(chain_id, contract)
        
        # Fallback s·ª≠a l·ªói d·ªØ li·ªáu 0
        if daily_total == 0 and daily_limit > 0: daily_total = daily_limit
        if daily_total < daily_limit: daily_total = daily_limit
        
        daily_onchain = daily_total - daily_limit
        chart_data = get_sparkline_data(chain_id, contract)

    return {
        "id": aid,
        "symbol": symbol,
        "name": item.get("name"),
        "icon": item.get("iconUrl"),
        "chain": chain_name,
        "chain_icon": item.get("chainIconUrl"),
        "contract": contract,
        
        # --- TR·∫†NG TH√ÅI & S·ª∞ KI·ªÜN ---
        "offline": is_offline,
        "listingCex": is_listing_cex,
        "status": status,
        "onlineTge": item.get("onlineTge", False),
        "onlineAirdrop": item.get("onlineAirdrop", False),
        # ----------------------------
        
        "mul_point": safe_float(item.get("mulPoint")),
        "listing_time": item.get("listingTime", 0),
        
        # D·ªØ li·ªáu c·∫≠p nh·∫≠t realtime t·ª´ API Nh·∫π
        "price": price,
        "change_24h": change_24h,
        "liquidity": liquidity,
        "market_cap": market_cap,
        "tx_count": tx_count,
        
        "volume": {
            "rolling_24h": vol_24h,
            "daily_total": daily_total,   # C√≥ th·ªÉ l·∫•y t·ª´ Cache ho·∫∑c API m·ªõi
            "daily_limit": daily_limit,
            "daily_onchain": daily_onchain
        },
        "chart": chart_data # C√≥ th·ªÉ l·∫•y t·ª´ Cache ho·∫∑c API m·ªõi
    }

# --- 4. MAIN ---
def fetch_data():
    global ACTIVE_SPOT_SYMBOLS, OLD_DATA_MAP
    
    start_time = time.time()
    print("üîí [SECURE MODE] B·∫Øt ƒë·∫ßu qu√©t d·ªØ li·ªáu th√¥ng minh...")
    
    # B∆Ø·ªöC 1: Load d·ªØ li·ªáu c≈© ƒë·ªÉ Cache
    OLD_DATA_MAP = load_old_data()
    print(f"üìÇ ƒê√£ t·∫£i {len(OLD_DATA_MAP)} token t·ª´ cache c≈©.")

    # B∆Ø·ªöC 2: L·∫•y danh s√°ch Spot th·ª±c t·∫ø (Cross-Check)
    ACTIVE_SPOT_SYMBOLS = get_active_spot_symbols()
    
    # B∆Ø·ªöC 3: G·ªçi API T·ªïng (API Nh·∫π)
    try:
        raw_res = fetch_with_retry(API_AGG_TICKER)
        raw_data = raw_res.get("data", [])
    except Exception as e:
        print(f"‚ùå L·ªói API T·ªïng: {e}")
        return

    tokens = []
    # Ch·ªâ x·ª≠ l√Ω token c√≥ volume > 0 ƒë·ªÉ nh·∫π g√°nh
    active_tokens = [t for t in raw_data if safe_float(t.get("volume24h")) > 0]
    inactive_tokens = [t for t in raw_data if safe_float(t.get("volume24h")) == 0]
    
    print(f"üìã T·ªïng API tr·∫£ v·ªÅ: {len(raw_data)}. Active c·∫ßn x·ª≠ l√Ω: {len(active_tokens)}")
    
    # B∆Ø·ªöC 4: Ch·∫°y ƒëa lu·ªìng th√¥ng minh
    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        results = list(executor.map(process_token_smart, active_tokens))

    for t in results:
        if t: tokens.append(t)
    
    # X·ª≠ l√Ω token r√°c (inactive)
    for t in inactive_tokens:
        basic = process_token_smart(t)
        if basic: tokens.append(basic)

    # S·∫Øp x·∫øp theo Volume gi·∫£i ƒë·∫•u
    final_sorted = sorted(tokens, key=lambda x: x["volume"]["daily_total"], reverse=True)

    # L∆∞u file
    data = {
        "last_updated": datetime.now().strftime("%H:%M %d/%m"),
        "tokens": final_sorted
    }
    
    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
        
    elapsed = time.time() - start_time
    print(f"‚úÖ HO√ÄN T·∫§T! T·ªïng th·ªùi gian: {elapsed:.2f}s")
    
    # DEBUG: Ki·ªÉm tra th·ª≠ 1 con Spot xem c√≥ b·ªã g·ªçi l·∫°i kh√¥ng
    spot_count = sum(1 for t in final_sorted if t["status"] == "SPOT")
    print(f"üìä Th·ªëng k√™: {spot_count} Token ƒëang l√† SPOT.")

if __name__ == "__main__":
    fetch_data()