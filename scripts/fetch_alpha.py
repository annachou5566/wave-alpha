import json
import os
import time
import urllib.parse
from datetime import datetime
from dotenv import load_dotenv
import requests 
import cloudscraper
import boto3 
from botocore.config import Config

# --- 1. C·∫§U H√åNH ---
load_dotenv()

# C·∫•u h√¨nh R2 / S3
R2_ACCESS_KEY_ID = os.getenv("R2_ACCESS_KEY_ID")
R2_SECRET_ACCESS_KEY = os.getenv("R2_SECRET_ACCESS_KEY")
R2_ENDPOINT_URL = os.getenv("R2_ENDPOINT_URL")
R2_BUCKET_NAME = os.getenv("R2_BUCKET_NAME")

PROXY_WORKER_URL = os.getenv("PROXY_WORKER_URL")
API_AGG_TICKER = os.getenv("BINANCE_INTERNAL_AGG_API")
API_AGG_KLINES = os.getenv("BINANCE_INTERNAL_KLINES_API")
API_PUBLIC_SPOT = "https://api.binance.com/api/v3/exchangeInfo"

ACTIVE_SPOT_SYMBOLS = set()
OLD_DATA_MAP = {}

# --- KH·ªûI T·∫†O K·∫æT N·ªêI R2 (OBJECT STORAGE) ---
def get_r2_client():
    if not R2_ACCESS_KEY_ID or not R2_SECRET_ACCESS_KEY:
        print("‚ö†Ô∏è Thi·∫øu R2 Credentials! Ki·ªÉm tra GitHub Secrets.")
        return None
    return boto3.client(
        's3',
        endpoint_url=R2_ENDPOINT_URL,
        aws_access_key_id=R2_ACCESS_KEY_ID,
        aws_secret_access_key=R2_SECRET_ACCESS_KEY,
        config=Config(signature_version='s3v4')
    )

# --- KH·ªûI T·∫†O SESSION REQUESTS ---
session = cloudscraper.create_scraper(
    browser={'browser': 'chrome', 'platform': 'windows', 'desktop': True}
)
session.headers.update({
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
    "Referer": "https://www.binance.com/en/alpha",
    "Origin": "https://www.binance.com",
    "Accept": "application/json"
})

# --- MAPPING L√ÄM R·ªêI D·ªÆ LI·ªÜU (ƒê·∫¶Y ƒê·ª¶ 100%) ---
KEY_MAP = {
    "id": "i", "symbol": "s", "name": "n", "icon": "ic",
    "chain": "cn", "chain_icon": "ci", "contract": "ct",
    "status": "st", "price": "p", "change_24h": "c",
    "market_cap": "mc", "liquidity": "l", "volume": "v",
    "rolling_24h": "r24", "daily_total": "dt",
    "daily_limit": "dl", "daily_onchain": "do",
    "chart": "ch", "listing_time": "lt", "tx_count": "tx",
    "offline": "off", "listingCex": "cex",
    "onlineTge": "tge",
    "onlineAirdrop": "air",
    # [M·ªöI] Th√™m Mul Point
    "mul_point": "mp"
}

def minify_token_data(token):
    minified = {}
    # 1. C√°c tr∆∞·ªùng c∆° b·∫£n
    minified[KEY_MAP["id"]] = token.get("id")
    minified[KEY_MAP["symbol"]] = token.get("symbol")
    minified[KEY_MAP["name"]] = token.get("name")
    minified[KEY_MAP["icon"]] = token.get("icon")
    
    # 2. C√°c tr∆∞·ªùng Chain (M·∫°ng l∆∞·ªõi) - ƒê√É B·ªî SUNG ƒê·∫¶Y ƒê·ª¶
    minified[KEY_MAP["chain"]] = token.get("chain")           # T√™n m·∫°ng
    minified[KEY_MAP["chain_icon"]] = token.get("chain_icon") # Logo m·∫°ng (C√°i b·∫°n ƒëang t√¨m)
    minified[KEY_MAP["contract"]] = token.get("contract")

    # 3. Tr·∫°ng th√°i & Gi√°
    minified[KEY_MAP["status"]] = token.get("status")
    minified[KEY_MAP["price"]] = token.get("price")
    minified[KEY_MAP["change_24h"]] = token.get("change_24h")
    minified[KEY_MAP["mul_point"]] = token.get("mul_point")   # [M·ªöI] ƒêi·ªÉm nh√¢n

    # 4. S·ªë li·ªáu t√†i ch√≠nh (√âp ki·ªÉu int cho g·ªçn n·∫øu s·ªë l·ªõn)
    minified[KEY_MAP["market_cap"]] = int(token.get("market_cap", 0))
    minified[KEY_MAP["liquidity"]] = int(token.get("liquidity", 0))
    minified[KEY_MAP["tx_count"]] = int(token.get("tx_count", 0))
    
    # 5. Th√¥ng tin Listing / Offline
    minified[KEY_MAP["listing_time"]] = token.get("listing_time")
    minified[KEY_MAP["offline"]] = 1 if token.get("offline") else 0
    minified[KEY_MAP["listingCex"]] = 1 if token.get("listingCex") else 0
    minified[KEY_MAP["onlineTge"]] = 1 if token.get("onlineTge") else 0
    minified[KEY_MAP["onlineAirdrop"]] = 1 if token.get("onlineAirdrop") else 0

    # 6. Volume (Gi·ªØ nguy√™n c·∫•u tr√∫c object con)
    vol = token.get("volume", {})
    minified[KEY_MAP["volume"]] = {
        KEY_MAP["rolling_24h"]: int(vol.get("rolling_24h", 0)),
        KEY_MAP["daily_total"]: int(vol.get("daily_total", 0)),
        KEY_MAP["daily_limit"]: int(vol.get("daily_limit", 0)),
        KEY_MAP["daily_onchain"]: int(vol.get("daily_onchain", 0))
    }
    
    # 7. Bi·ªÉu ƒë·ªì
    minified[KEY_MAP["chart"]] = token.get("chart", [])
    
    return minified

# --- H√ÄM G·ªåI API ---
def fetch_smart(target_url, retries=3):
    is_render = "onrender.com" in (PROXY_WORKER_URL or "")
    if not target_url or "None" in target_url: return None

    for i in range(retries):
        if PROXY_WORKER_URL:
            try:
                encoded_target = urllib.parse.quote(target_url, safe='')
                proxy_final_url = f"{PROXY_WORKER_URL}?url={encoded_target}"
                current_timeout = 60 if (is_render and i == 0) else 30
                res = session.get(proxy_final_url, timeout=current_timeout)
                if res.status_code == 200:
                    data = res.json()
                    if isinstance(data, dict):
                        if "symbols" in data: return data 
                        if data.get("code") == "000000": return data
                elif res.status_code == 502: time.sleep(3)
            except: pass
        
        try:
            res = session.get(target_url, timeout=15)
            if res.status_code == 200:
                data = res.json()
                if "symbols" in data: return data
                if data.get("code") == "000000": return data
        except: pass
        time.sleep(1)
    return None

def safe_float(v):
    try: return float(v) if v else 0.0
    except: return 0.0

# --- T·∫¢I DATA C≈® T·ª™ R2 (THAY V√å LOAD LOCAL) ---
def load_old_data_from_r2(r2_client):
    if not r2_client: return {}
    try:
        # T·∫£i file market-data.json t·ª´ R2 v·ªÅ b·ªô nh·ªõ
        obj = r2_client.get_object(Bucket=R2_BUCKET_NAME, Key='market-data.json')
        data = json.loads(obj['Body'].read().decode('utf-8'))
        
        # V√¨ d·ªØ li·ªáu c≈© tr√™n R2 ƒë√£ b·ªã Minify (l√†m r·ªëi), ta c·∫ßn map ng∆∞·ª£c l·∫°i ID
        # ƒë·ªÉ code logic hi·ªÉu ƒë∆∞·ª£c. (Tuy nhi√™n, logic check limit ch·ªß y·∫øu c·∫ßn ID,
        # n·∫øu minify ID v·∫´n gi·ªØ nguy√™n th√¨ OK).
        # ·ªû ƒë√¢y ƒë∆°n gi·∫£n h√≥a: N·∫øu ƒë√£ minify th√¨ ID l√† key "i"
        
        tokens = data.get('data', [])
        mapped_data = {}
        for t in tokens:
            # Map key 'i' (minified) ho·∫∑c 'id' (legacy)
            tid = t.get('i') or t.get('id')
            if tid: mapped_data[tid] = t
            
        return mapped_data
    except Exception as e:
        print(f"‚ö†Ô∏è Kh√¥ng t·∫£i ƒë∆∞·ª£c cache t·ª´ R2 (L·∫ßn ƒë·∫ßu ch·∫°y?): {e}")
        return {}

def get_active_spot_symbols():
    try:
        print("‚è≥ Check Spot Market...", end=" ", flush=True)
        data = fetch_smart(API_PUBLIC_SPOT)
        if data and "symbols" in data:
            res = {s["baseAsset"] for s in data["symbols"] if s["status"] == "TRADING"}
            print(f"OK ({len(res)})")
            return res
    except: pass
    return set()

def fetch_details_optimized(chain_id, contract_addr):
    if not API_AGG_KLINES: return 0, 0, 0, []
    no_lower_chains = ["CT_501", "CT_784"]
    clean_addr = str(contract_addr)
    if chain_id not in no_lower_chains: clean_addr = clean_addr.lower()
    
    base_url = f"{API_AGG_KLINES}?chainId={chain_id}&interval=1d&limit=30&tokenAddress={clean_addr}"
    d_total, d_limit = 0.0, 0.0
    chart_data = []

    try:
        res_limit = fetch_smart(f"{base_url}&dataType=limit")
        if res_limit and res_limit.get("data") and res_limit["data"].get("klineInfos"):
            k_infos = res_limit["data"]["klineInfos"]
            if k_infos: d_limit = safe_float(k_infos[-1][5])
    except: pass

    try:
        res_agg = fetch_smart(f"{base_url}&dataType=aggregate")
        if res_agg and res_agg.get("data") and res_agg["data"].get("klineInfos"):
            k_infos = res_agg["data"]["klineInfos"]
            if k_infos:
                d_total = safe_float(k_infos[-1][5])
                chart_data = [{"p": safe_float(k[4]), "v": safe_float(k[5])} for k in k_infos]
    except: pass

    d_market = d_total - d_limit
    if d_market < 0: d_market = 0 
    return d_total, d_limit, d_market, chart_data

def process_single_token(item):
    aid = item.get("alphaId")
    if not aid: return None

    vol_rolling = safe_float(item.get("volume24h"))
    symbol = item.get("symbol")
    contract = item.get("contractAddress")
    chain_id = item.get("chainId")
    is_offline = item.get("offline", False)
    is_listing_cex = item.get("listingCex", False)
    
    status = "ALPHA"
    need_limit_check = False 
    if is_offline:
        if is_listing_cex or symbol in ACTIVE_SPOT_SYMBOLS: status = "SPOT"
        else:
            status = "PRE_DELISTED"
            need_limit_check = True

    # --- [M·ªöI] LOGIC CACHE: CH·∫∂N TOKEN ƒê√É CH·∫æT T·ª™ L·∫¶N TR∆Ø·ªöC ---
    # M·ª•c ƒë√≠ch: N·∫øu l·ªãch s·ª≠ (OLD_DATA_MAP) ghi nh·∫≠n l√† DELISTED th√¨ b·ªè qua lu√¥n.
    # L∆∞u √Ω: OLD_DATA_MAP d√πng key ƒë√£ minify (v√≠ d·ª•: KEY_MAP["status"] = "st")
    if OLD_DATA_MAP and aid in OLD_DATA_MAP:
        old_item = OLD_DATA_MAP[aid]
        # Ki·ªÉm tra tr·∫°ng th√°i c≈©
        if old_item.get(KEY_MAP["status"]) == "DELISTED":
            status = "DELISTED"
            need_limit_check = False  # T·∫Øt c·ªù check limit ƒë·ªÉ kh√¥ng chui v√†o should_fetch
    # -----------------------------------------------------------

    should_fetch = False
    if vol_rolling > 0 and (status == "ALPHA" or status == "PRE_DELISTED"):
        should_fetch = True
    
    daily_total, daily_limit, daily_onchain = 0.0, 0.0, 0.0
    chart_data = []
    
    # Logic Cache: C·∫ßn x·ª≠ l√Ω kh√©o h∆°n v√¨ key cache ƒë√£ b·ªã minify
    # Nh∆∞ng ƒë·ªÉ an to√†n cho phi√™n b·∫£n n√†y, ta t·∫°m ∆∞u ti√™n fetch m·ªõi.
    
    if should_fetch:
        print(f"üì° {symbol}...", end=" ", flush=True)
        try:
            d_t, d_l, d_m, chart = fetch_details_optimized(chain_id, contract)
            daily_total, daily_limit, daily_onchain = d_t, d_l, d_m
            chart_data = chart
            
            if need_limit_check:
                if daily_limit > 0:
                    status = "ALPHA"
                    print("‚úÖ ALIVE")
                else:
                    status = "DELISTED"
                    print("‚ùå DEAD")
            else: print("OK")
            if daily_total <= 0: daily_total = vol_rolling
        except Exception as e:
            print(f"‚ö†Ô∏è Err: {e}")
            daily_total = vol_rolling
            if need_limit_check: status = "DELISTED"
    else:
        daily_total = vol_rolling
        if status == "PRE_DELISTED": status = "DELISTED"
        
        # [M·ªöI] T√°i s·ª≠ d·ª•ng Chart c≈© n·∫øu c√≥ (ƒë·ªÉ kh√¥ng b·ªã m·∫•t bi·ªÉu ƒë·ªì khi skip fetch)
        if status == "DELISTED" and OLD_DATA_MAP and aid in OLD_DATA_MAP:
            old_item = OLD_DATA_MAP[aid]
            if old_item.get(KEY_MAP["chart"]):
                chart_data = old_item.get(KEY_MAP["chart"])

    return {
        "id": aid, "symbol": symbol, "name": item.get("name"),
        "icon": item.get("iconUrl"), "chain": item.get("chainName", ""),
        "chain_icon": item.get("chainIconUrl"), "contract": contract,
        "offline": is_offline, "listingCex": is_listing_cex, "status": status,
        "onlineTge": item.get("onlineTge", False),
        "onlineAirdrop": item.get("onlineAirdrop", False),
        "mul_point": safe_float(item.get("mulPoint")),
        "listing_time": item.get("listingTime", 0),
        "tx_count": safe_float(item.get("count24h")),
        "price": safe_float(item.get("price")),
        "change_24h": safe_float(item.get("percentChange24h")),
        "liquidity": safe_float(item.get("liquidity")),
        "market_cap": safe_float(item.get("marketCap")),
        "volume": {
            "rolling_24h": vol_rolling, "daily_total": daily_total,
            "daily_limit": daily_limit, "daily_onchain": daily_onchain
        },
        "chart": chart_data
    }

def fetch_data():
    global ACTIVE_SPOT_SYMBOLS, OLD_DATA_MAP
    start = time.time()
    
    r2 = get_r2_client()
    if not r2: return

    OLD_DATA_MAP = load_old_data_from_r2(r2)
    ACTIVE_SPOT_SYMBOLS = get_active_spot_symbols()
    
    print("‚è≥ List...", end=" ", flush=True)
    try: raw_res = fetch_smart(API_AGG_TICKER)
    except: return
    if not raw_res: return
    
    raw_data = raw_res.get("data", [])
    print(f"Done ({len(raw_data)})")

    target_tokens = raw_data
    target_tokens.sort(key=lambda x: safe_float(x.get("volume24h")), reverse=True)
    
    results = []
    print(f"üöÄ Processing {len(target_tokens)} Tokens (R2 Storage Mode)...")
    
    for t in target_tokens:
        r = process_single_token(t)
        if r: results.append(r)

    results.sort(key=lambda x: x["volume"]["daily_total"], reverse=True)

    # --- MINIFY DATA ---
    print(f"üîí Minifying...")
    minified_results = [minify_token_data(t) for t in results]

    final_output = {
        "meta": {
            "u": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "t": len(minified_results),
            "c": "WaveAlpha Data"
        },
        "data": minified_results
    }
    
    json_str = json.dumps(final_output, ensure_ascii=False, separators=(',', ':'))

    # --- UPLOAD TO CLOUDFLARE R2 ---
    print("‚òÅÔ∏è Uploading to Cloudflare R2...")
    try:
        # 1. Upload File M·ªõi Nh·∫•t
        r2.put_object(
            Bucket=R2_BUCKET_NAME,
            Key='market-data.json',
            Body=json_str.encode('utf-8'),
            ContentType='application/json',
            CacheControl='max-age=60' # Cache 1 ph√∫t
        )
        print("‚úÖ Uploaded market-data.json")

        # 2. Upload File L·ªãch S·ª≠
        today_str = datetime.now().strftime("%Y-%m-%d")
        r2.put_object(
            Bucket=R2_BUCKET_NAME,
            Key=f'history/{today_str}.json',
            Body=json_str.encode('utf-8'),
            ContentType='application/json'
        )
        print(f"‚úÖ Uploaded history/{today_str}.json")

    except Exception as e:
        print(f"‚ùå R2 Upload Failed: {e}")

    print(f"üèÅ DONE! Total: {time.time()-start:.1f}s")

if __name__ == "__main__":
    fetch_data()
